<?xml version="1.0" encoding="utf-8" ?>
<rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:wfw="http://wellformedweb.org/CommentAPI/" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>The Rootavish Files</title>
    <atom:link href="http://localhost:8080/feed.xml" rel="self" type="application/rss+xml"></atom:link>
    <link>http://localhost:8080</link>
    <description>Set up for and currently being used exclsively for doing weekly write ups as a GSoC student for ScrapingHub</description>
    <pubDate>Sat, 23 Jul 2016 05:30:00 +0630</pubDate>
    <generator>Wintersmith - https://github.com/jnordberg/wintersmith</generator>
    <language>en</language>
    <item>
      <title>Formalising the Benchmark Suite, Some More Unit Tests and Backward Compatibility Changes</title>
      <link>http://localhost:8080/articles/formalising-benchmark-suite-and-compatibility/</link>
      <pubDate>Sat, 23 Jul 2016 05:30:00 +0630</pubDate>
      <guid isPermaLink="true">http://localhost:8080/articles/formalising-benchmark-suite-and-compatibility/</guid>
      <author></author>
      <description>&lt;p&gt;In the past two weeks I focused my efforts on finalizing the benchmarking suite and improving test coverage. &lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;more&quot;&gt;&lt;/span&gt;
From what &lt;code&gt;Codecov&lt;/code&gt; says, we’re 83% of the way there regarding test coverage. As far as the performance of the new signals is concerned, from what the testing shows I gathered that the new signal API always takes less than half the time that is required by the old signal API for both signal connection and the actual sending of the signal. &lt;/p&gt;
&lt;p&gt;This is attributed mostly to the fact that a lot of time that was previously used up by running a
combo of the &lt;code&gt;getAllReceivers&lt;/code&gt; and &lt;code&gt;liveReceivers&lt;/code&gt; functions together everytime was taking up a huge amount of time and was the bottleneck
to the process. As it currently stands, we’re not using the caching mechanism of the library, i.e. have &lt;code&gt;use_caching&lt;/code&gt; set to false always because
the receivers which do not connect to a specific sender but rather to all require me to find a suitable key for them that can be &lt;code&gt;weakref&lt;/code&gt; ref’d to
make the entry in the &lt;code&gt;WeakKeyDictionary&lt;/code&gt;. But enough about that, back to benchmarking.  &lt;/p&gt;
&lt;p&gt;So for the benchmarking process, &lt;code&gt;Djangobech&lt;/code&gt; the &lt;code&gt;Django&lt;/code&gt;
benchmarking library, does not benchmark the signals currently and the same is still on the &lt;code&gt;TODO&lt;/code&gt; list in the project. They however, did provide
me with some excellent modules that I used to write the scrapy benchmarking suite for signals. I would leave a link to the same here, but currently
I’m in a discussion with my mentor on where to include them, as including them in the repo would require that we still keep &lt;code&gt;pyDispatcher&lt;/code&gt; as a dependency
as it is required to perform a raw apples to apples comparison of the signal code. In this post I’m also sharing results that I got using Robert Kern’s &lt;code&gt;line_profiler&lt;/code&gt; module.&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/articles/formalising-benchmark-suite-and-compatibility/line_profiler_output.png&quot; alt=&quot;line profiler output&quot;&gt;.&lt;/p&gt;
&lt;p&gt;As for the compatibility changes this cycle, I added support for the old style scrapy signals, which were just standard python objects. In similar
fashion to how I implemented backward compatiblity for receivers without keyword arguments, I proxied the signals through the signal manager to implement backward compatability for the objects. With that the new signals can be safely integrated into scrapy with no worries about breaking legacy code. In the coming weeks, I plan on working on finishing test coverage, maybe adding some signal benchmarks to &lt;code&gt;scrapy bench&lt;/code&gt; and doing documentation.&lt;/p&gt;
</description>
    </item>
    <item>
      <title>Benchmarking and Unit Testing</title>
      <link>http://localhost:8080/articles/benchmarking-and-unit-testing/</link>
      <pubDate>Fri, 08 Jul 2016 05:30:00 +0630</pubDate>
      <guid isPermaLink="true">http://localhost:8080/articles/benchmarking-and-unit-testing/</guid>
      <author></author>
      <description>&lt;p&gt;Hi, sorry this post a couple days later than warranted, but there’s been some
work that required being taken care of. Since the end of the midterm evaluations,
efforts were concentrated into taking care of issues such as compatability with the
older API of scrapy, unit testing and benchmarking the signal performance with the
new API and looking for places where further optimizations are required.&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;more&quot;&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;One such area I identified was that the &lt;code&gt;use_caching&lt;/code&gt; functionality of the dispatch lib
was always going unused because of the presence of NoneType objects when a receiver wants
to be triggered on signals sent by any sender. I’m looking into how we can make that usable,
as using the caching we can make the Signals perform even better, since majority of the time taken
by the &lt;code&gt;send_catch_log&lt;/code&gt; function is in the &lt;code&gt;_liveReceivers&lt;/code&gt; method of the &lt;code&gt;Signal&lt;/code&gt; class. This
method would have a constant time look up had it been for the cache.&lt;/p&gt;
&lt;p&gt;Here’s some preliminary results of the benchmarks, obtained by running the benchmarking spider with
–profile as with:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;bash $scrapy bench --profile -o new_profile.cprofile&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Here’s the visualizations of the output obtained using &lt;code&gt;pyprof2calltree&lt;/code&gt;:
&lt;img src=&quot;/articles/benchmarking-and-unit-testing/new_api.png&quot; alt=&quot;new api called&quot;&gt;&lt;/p&gt;
&lt;p&gt;The old API:
&lt;img src=&quot;/articles/benchmarking-and-unit-testing/old_api.png&quot; alt=&quot;old api called&quot;&gt;&lt;/p&gt;
&lt;p&gt;As we can see, the difference between the two API’s is clear in terms of performance in that the older
API is clearly taking significantly longer time and a lot more cycles than the new API, while the helper methods
of the old dispatcher too are taking their fair share of time(results not in picture). Such is not the case anymore.
Next, I seek to formalize these results in the form of a benchmarking suite and look for ways to improve the
performance even further. Until next time.&lt;/p&gt;
</description>
    </item>
    <item>
      <title>Midterm Evaluations</title>
      <link>http://localhost:8080/articles/midterm-evaluation/</link>
      <pubDate>Fri, 24 Jun 2016 05:30:00 +0630</pubDate>
      <guid isPermaLink="true">http://localhost:8080/articles/midterm-evaluation/</guid>
      <author></author>
      <description>&lt;h3 id=&quot;what-s-done&quot;&gt;What’s done&lt;/h3&gt;
&lt;p&gt;In the time that has passed since I last posted one of these, I managed to get a prototype of scrapy to work using the new Signals API. This introduced two very significant API changes into Scrapy.&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;more&quot;&gt;&lt;/span&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;All Signals now need to be objects of the &lt;code&gt;scrapy.dispatch.Signal&lt;/code&gt; class instead of the generic python &lt;code&gt;object&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;All signal handlers must now receive &lt;code&gt;**kwargs&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The first change would not affect the existing extensions/3^rd party plugins much since declaring new signals is not something for the most part extensions do, and using &lt;code&gt;PyDispatcher&lt;/code&gt; to call the signals instead of the &lt;code&gt;SignalManager&lt;/code&gt; class has long been deprecated in Scrapy. To accomodate this, the Scrapy &lt;code&gt;SignalManager&lt;/code&gt; has not yet been phased out and would still be functional, although possibly deprecated depending on how the performance benchmarks work out, and whether avoiding the overhead for the method calls is required.  &lt;/p&gt;
&lt;p&gt;The second of these changes however, affects the majority of these extensions and requires that we accomodate in someway. The solution required accomodating the &lt;code&gt;RobustApply&lt;/code&gt; method of PyDispatcher in Scrapy, this method however would considerably affect the performance of the module, and so in order to have the faster signals one would be required to use handlers with keyword arguments.  &lt;/p&gt;
&lt;p&gt;The API was also modified to accomodate twisted &lt;code&gt;deferred&lt;/code&gt; objects to be returned, and the error handling changed to use the &lt;code&gt;Failure&lt;/code&gt; class from &lt;code&gt;twisted.deferred&lt;/code&gt;.  &lt;/p&gt;
&lt;p&gt;The new module has also been unit tested for the most part, with some tests borrowed from Django since they’re the original authors of this signals API.
I’m currently working on the benchmark suite, writing spiders that use non-standard signals and calls to test the performance. Eariler, signaling through &lt;code&gt;send_catch_log&lt;/code&gt; used to be the biggest bottleneck requiring 5X the time required for HTML parsing. Any improvements we can do on that, the better although ideally I would like if we could make it so that signals are no longer the bottleneck to the crawling.  &lt;/p&gt;
&lt;p&gt;&lt;strong&gt; The following section is under construction. &lt;/strong&gt;&lt;/p&gt;
&lt;h3 id=&quot;what-needs-to-be-done&quot;&gt;What needs to be done&lt;/h3&gt;
&lt;p&gt;Following the midterms, the highest priority would be to complete the bechmark suite so we know the viability of the approach we have used thus far and where to proceed from here. In case the results obtained are satisfactory, we shall then continue to make backward compatibility fixes and re-writing algortihms that are still not as efficient as they can be and look to maximize performance. We can continue on to provide full backward compatibility with object() like signals, however that would come with the trade-off that the performance of them would be more or less same as that of what was previously achieveable from the API.  &lt;/p&gt;
&lt;p&gt;Another major requirement would be for me to write good documentation of these parts, since these are essential to anybody writing an extension. We would also need to be on the lookout for regressions, if any.&lt;/p&gt;
&lt;p&gt;~ Avishkar&lt;/p&gt;
</description>
    </item>
    <item>
      <title>Rewriting Scrapy Signals, Part I</title>
      <link>http://localhost:8080/articles/rewriting-scrapy-signals/</link>
      <pubDate>Fri, 10 Jun 2016 05:30:00 +0630</pubDate>
      <guid isPermaLink="true">http://localhost:8080/articles/rewriting-scrapy-signals/</guid>
      <author></author>
      <description>&lt;p&gt;In this report, I shall present a brief summary of the work done upto this point. To iterate, the end goal of the project is to improve the efficiency
of Scrapy’s signaling API. For this purpose the idea was to use Django’s signaling mechanism which claims to make the signals 90% faster. Consequently, according to plan the first step was to port &lt;code&gt;django.dispatch&lt;/code&gt;, modify it to our needs and create the &lt;code&gt;scrapy.dispatch&lt;/code&gt;. About a week and a half was spent on that front in first understanding the way that library is written, and what would changes needed to go into making the same work with Scrapy, and then actually making those changes. &lt;code&gt;django.dispatch&lt;/code&gt; refactored much of the code of &lt;code&gt;PyDispatcher&lt;/code&gt; into a &lt;code&gt;Signal&lt;/code&gt; class. It also introduced a caching mechanism for receivers and introduced the &lt;code&gt;weakref&lt;/code&gt; module into the code.    &lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;more&quot;&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;send_robust&lt;/code&gt; method was used as a starting point for the &lt;code&gt;send_robust_deferred&lt;/code&gt; call to incorporate methods returning deferred calls, this was required for re-writing the &lt;code&gt;scrapy.utils.signal&lt;/code&gt; module which has the &lt;code&gt;send_catch_log&lt;/code&gt; and &lt;code&gt;send_catch_log_deferred&lt;/code&gt; modules. These methods are relative inefficient and tend to bottleneck the scraping process, and were one of the reasons behind the re-write of signals. The next step involved was to re-define the core signals at the heart of Scrapy as instances of &lt;code&gt;scrapy.Signal&lt;/code&gt; instead of generic &lt;code&gt;object&lt;/code&gt;class. The &lt;code&gt;signalManager&lt;/code&gt; class also needed to be changed however the API of the same was kept consistent for backward compatibiliy.&lt;/p&gt;
&lt;p&gt;You can track the progress and check out how the project is coming along &lt;a href=&quot;https://github.com/rootavish/scrapy/tree/signal-rewrite&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;https://github.com/scrapy/scrapy/pull/2030&quot;&gt;here&lt;/a&gt; although I must admit I do not push frequently and most commits are either local or small test pieces written outside the mainline code. I shall however, be pushing a working prototype by the end of Monday so that’s there to look out for.&lt;/p&gt;
</description>
    </item>
    <item>
      <title>Community Bonding</title>
      <link>http://localhost:8080/articles/community-bonding/</link>
      <pubDate>Sun, 22 May 2016 05:30:00 +0630</pubDate>
      <guid isPermaLink="true">http://localhost:8080/articles/community-bonding/</guid>
      <author></author>
      <description>&lt;p&gt;Hey there, this is the first in the series of posts aimed at documenting my experience as a GSoC 2016 student for regular reporting to the org,
and for my own reference to have something to look back on in retrospect. For the first post, let me start with spouting off a little about myself,
and then I’ll talk about how the experience has been to date.&lt;/p&gt;
&lt;p&gt;&lt;span class=&quot;more&quot;&gt;&lt;/span&gt;
I’m a(or rather was) a final year student of Computer Engineering at a college in New Delhi, and I’ve been a part of the GSoC program back in 2014,
where I worked for &lt;a href=&quot;http://mate-desktop.org&quot;&gt;MATE desktop&lt;/a&gt;. For the last year or so, I’ve been dabbling in data science and machine learning, and came
to know of Scrapy when I used it for one such project early on to scrape opinions from an e-commerce site. Fast forward a year and casually browsing through
PSF’s list of GSoC organizations I got to know that these guys were also participating and decided to give it a shot. Given how hectic the schedule was for me back
in February, I would be lying if I said that the reason for my selection was me being some sort of a big-shot programmer who came in all guns blazing. I approached
this as passively as one possibly could, it was due to the efforts of the ScrapingHub suborg admin Paul who showed interst in my proposal, gave me an interview
and put me to work on a bug which also made me familiar with the actual inner workings of Scrapy that I was able to gain footing on the project.  &lt;/p&gt;
&lt;p&gt;My project for this summer is going to deal with re-factoring the Scrapy signaling API, in an effort to move away from the PyDispatcher library which would greatly
enchance the performance of signals. Django moved away from PyDispatcher in 2001, and they reportedly observed an increase of upto 90% in efficiency. I intend to build
off their work and assume we would see similar results in Scrapy.  &lt;/p&gt;
&lt;p&gt;The Scrapy community are a really active lot, and my “community bonding” started just a couple days into the announcement of me being selected for the project, which is good
because I’ve had exams for the past couple of weeks or so and was AFK for a major part of them(of course informing my mentors about the same first). I had a video chat with my
mentor Jakob where we figured out how reporting etc. would work for the summer, our next chat is scheduled for the 24^th, 25^th where we shall discuss how the actual implementation
of the project will be and what plans I have for the same. I also finalised the work on a bug I was working on and had submitted in the form of a patch as a part of my proposal.
The Scrapy community is really responsive, and I’m honored to be a part of it and to be working with all the people here. I hope my work is up to their standards at the end of this.
Since there is not much Technical content to write about at this point, with the coding period having not yet started so we’ll keep this one short, I’ll bore you with the
technical details in the next one. Thanks for reading, the next post will go up on Sunday the 28th. Signing off.&lt;/p&gt;
</description>
    </item>
    <item>
      <title>The First Post</title>
      <link>http://localhost:8080/articles/first-post/</link>
      <pubDate>Sat, 30 Apr 2016 05:30:00 +0630</pubDate>
      <guid isPermaLink="true">http://localhost:8080/articles/first-post/</guid>
      <author></author>
      <description>&lt;p&gt;This blog will contain weekly reports I write as a GSoC student for Scrapinghub.&lt;/p&gt;
</description>
    </item>
  </channel>
</rss>